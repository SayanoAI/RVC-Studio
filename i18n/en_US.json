{
    "inference.save.button": "Save Options",
    "inference.refresh_data.button": "Refresh Data",
    "inference.clear_data.button": "Unload Model",
    "inference.load_model.button": "Load Model",
    "inference.one_click.button": "1-Click VC",
    "inference.song.selectbox": "Select a song file",
    "inference.voice.selectbox": "Select a voice model",

    "inference.split_vocals": "Extract Vocals",
    "inference.split_vocals.expander": "Extract Vocals Options",
    "inference.preprocess_model": "Choose a preprocessing model (sequential)",
    "inference.model_paths": "Choose a vocal extraction model (parallel)",
    "inference.postprocess_model": "Chose a postprocessing model (sequential)",
    "inference.device": "Device",
    "inference.format": "Audio Format",
    "inference.merge_type": "Merge Type",
    "inference.agg": "Aggressiveness in isolating vocals",
    "inference.use_cache": "Reduce future processing time by saving results to disk",

    "inference.convert_vocals": "Change Vocals",
    "inference.convert_vocals.expander": "Change Vocals Options",
    "inference.convert_vocals.expander.form_submit_button": "Save Options",
    
    "inference.f0_up_key": "Recommended +7 or +12 key for male to female conversion, and -12 or -5 key for female to male conversion. If the sound range goes too far and the voice is distorted, you can also adjust it to the appropriate range by yourself.",
    "inference.f0_method": "Select the pitch extraction algorithm ('crepe': better quality but GPU intensive, 'rmvpe': fast and best quality, and little GPU requirement, others: use them if you really want to...)",
    "inference.f0_autotune": "Enable autotuning of extracted pitch",
    "inference.filter_radius": "If >=3: apply median filtering to the harvested pitch results. The value represents the filter radius and can reduce breathiness.",
    "inference.index_rate": "Search feature ratio (controls accent strength, too high has artifacting):",
    "inference.resample_sr": "Resample the output audio in post-processing to the final sample rate. Set to 0 for no resampling:",
    "inference.rms_mix_rate": "Adjust the volume envelope scaling. Closer to 0, the more it mimicks the volume of the original vocals. Can help mask noise and make volume sound more natural when set relatively low. Closer to 1 will be more of a consistently loud volume:",
    "inference.protect": "Protect voiceless consonants and breath sounds to prevent artifacts such as tearing in electronic music. Set to 0.5 to disable. Decrease the value to increase protection, but it may reduce indexing accuracy:",
    
    "inference.download.button": "Download Song",

    "training.preprocess_data.title": "Process Data",
    "training.preprocess_data.text": "Step 1: Fill in the experimental configuration. Experimental data is stored in the 'logs' folder, with each experiment having a separate folder. Manually enter the experiment name path, which contains the experimental configuration, logs, and trained model files.",
    "training.exp_dir": "Enter the voice model name:",
    "training.sr": "Target sample rate:",
    "training.if_f0": "Whether the model has pitch guidance (required for singing, optional for speech):",
    "training.version": "Version:",
    "training.device": "Device:",
    "training.n_threads": "Number of CPU processes used for pitch extraction and data processing:",
    "training.extract_features.title": "Extract Features",
    "training.extract_features.text": "Step 2: Automatically traverse all files in the training folder that can be decoded into audio and perform slice normalization. Generates 2 wav folders in the experiment directory. Currently, only single-singer/speaker training is supported.",
    "training.preprocess_data.trainset_dir": "Enter the path of the training folder:",
    "training.sid": "Please specify the speaker/singer ID:",
    "training.preprocess_data.submit": "1. Process data",
    "training.train_model.gpus": "Enter the GPU index(es) separated by '-', e.g., 0-1-2 to use GPU 0, 1, and 2:",
    "training.f0method": "Select the pitch extraction algorithm ('crepe': better quality but GPU intensive, 'rmvpe': best quality, and little GPU requirement)",
    "training.extract_features.submit": "2. Feature extraction",
    "training.train_model.title": "Train Model",
    "training.train_model.text": "Step 3: Fill in the training settings and start training the model and index",
    "training.save_every_epoch": "Save frequency (save_every_epoch):",
    "training.total_epoch": "Total training epochs (total_epoch):",
    "training.batch_size": "Batch size per GPU:",
    "training.if_save_latest": "Save only the latest '.ckpt' file to save disk space",
    "training.if_save_best": "Save only the best model (lowest overall loss) to the 'logs' folder",
    "training.if_cache_gpu": "Cache all training sets to GPU memory. Caching small datasets (less than 10 minutes) can speed up training, but caching large datasets will consume a lot of GPU memory and may not provide much speed improvement",
    "training.if_save_every_weights": "Save a small final model to the 'logs' folder at each save point",
    "training.pretrained_G": "Load pre-trained base model G path:",
    "training.pretrained_D": "Load pre-trained base model D path:",
    "training.train_model.submit": "3. Train model",
    "training.train_index.submit": "4. Train feature index",
    "training.train_speaker.submit": "Train Optional Speaker Embedding",
    "training.one_click": "One-click training",
    "training.gpus": "Which CUDA GPU to use for training (leave empty to use all of them)",

    "process.pids": "Active Processes",
    "process.kill_all_pids": "Close All Processes",
    "process.kill_one_pid": "Stop",

        "tts.inference": "Text to Speech",
        "tts.options": "TTS Options",
        "tts.model.selectbox": "TTS Models"
}
